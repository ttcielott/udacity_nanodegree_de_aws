Spark uses lazy evaluation, which means that it doesn't execute transformations like the map step immediately. Instead, it builds up a directed acyclic graph (DAG) of the operations to be performed on the data. This allows Spark to optimize the execution plan and avoid unnecessary computations.

When you call an action like collect(), Spark triggers the execution of the DAG and performs the necessary computations to produce the final result. In the case of collect(), it collects the results from all the partitions of the RDD and returns them as a single list on the driver node.